---
title: "HW13_Sabrina Liu"
output: html_document
---

#4.5.4(a)
#The data for this problem were collected between 3:25p.m. and 4:05p.m. on a non-holiday weekday.This would affect our conclusion of this problem case, since for non-holiday weekday, the traffic situations are different. Moreover, any specific time session could affect the observation. 

#4.5.4(b)
#These obserbations are independent.The sample mean is y ̄ = 3.9. The sample variance, s2 = 4.3, is fairly close to the mean, as is expected if the car count random variable follows a Poisson distribution. A frequency distribution and a histogram of the data are produced by the code below.
```{r}
stoplight <- read.csv(file  = "stoplight.csv")
head(stoplight)
mean(stoplight$vehicles)
var(stoplight$vehicles)

# Frequencies
table(stoplight$vehicles) #Note that y = 0, 1, ..., 8 all have positive counts
rel.freq <- table(stoplight$vehicles)/length(stoplight$vehicles)
rel.freq2 <- c(rel.freq, rep(0, times = 7))

# Poisson calculations
y <- 0:15
prob <- round(dpois(x = y, lambda = mean(stoplight$vehicles)), 4)

alpha <- 0.05
n <- length(stoplight$vehicles)
mu.hat <- mean(stoplight$vehicles)

# Observed and Poisson
poisson_table = data.frame(y, prob, rel.freq = rel.freq2)
poisson_table

# Plot
x11(width = 7, height = 6, pointsize = 12)
plot(x = y - 0.1, y = prob, type = "h", ylab = "Probability", xlab = "Number of vehicles", lwd = 2,
     xaxt = "n")
axis(side = 1, at = 0:15)
lines(x = y + 0.1, y = rel.freq2, type = "h", lwd = 2, lty = "solid", col = "red")
abline(h = 0)
legend(x = 9, y = 0.15, legend = c("Poisson", "Observed"), lty = c("solid", "solid"), lwd = c(2,2), col = c("black", "red"), bty = "n")
```
#We can calculate this difference, and use a bar plot to visualize the difference. Accoridng to our bar plot, in most cases, there were no such difference between those two probabilities. If the probabilities of one variable remains fixed, regardless of whether we condition on another variable, then the two variables are independent. 

#4.5.4(c)
```{r}
prob_9 = 1 - ppois(q = 8, lambda = mu.hat)
prob_9
```
#This is the probability for one stoplight cycle.

#4.5.4(d)
```{r}
1 - dbinom(x = 0, size = 60, prob = prob_9)
```

#4.5.13(a)
#The population of inference is a Starbucks location in downtown Lincoln, NE between 8:00 AM to 8:00 AM in weekdays. 

#4.5.13(b)
```{r}
library(ggplot2)
starbucks <- read.csv(file  = "starbucks.csv")
starbucks = as.data.frame(starbucks)
starbucks
starbucks$Day <- factor(starbucks$Day)
ggplot(starbucks, aes(Day, Count)) + geom_point()
```

#4.5.13(c)
```{r}
library(mcprofile)
summary(mod.fit1 <- glm(Count ~ Day, family="poisson", data=starbucks))

#LRT
anova(mod.fit1)
mean(starbucks$Count)
var(starbucks$Count)

# Frequencies
table(starbucks$Count) #Note that y = 0, 1, ..., 8 all have positive counts
rel.freq <- table(starbucks$Count)/length(starbucks$Count)
rel.freq2 <- c(rel.freq, rep(0, times = 6))

# Poisson calculations
y <- 0:15
prob <- round(dpois(x = y, lambda = mean(starbucks$Count)), 4)

# Observed and Poisson
poisson_table = data.frame(y, prob, rel.freq = rel.freq2)
poisson_table

alpha<-0.05
n<-length(starbucks$Count)
mu.hat<-mean(starbucks$Count)
mu.hat + qnorm(p = c(alpha/2, 1-alpha/2))*sqrt(mu.hat/n)
exp(log(mu.hat) + qnorm(p = c(alpha/2, 1-alpha/2)) * sqrt(1/(mu.hat*n)))

#Estimate the ratio of means comparing each pair of the days of the week and compute 95% confidence intervals for these same comparisons. 
t.test(x = starbucks$Count, conf.level = 0.95)

#estimated mean number of customers for each day of the week using the model
aggregate(starbucks$Coun, list(starbucks$Day), FUN=mean)

alpha = 0.05
pred.data <- data.frame(Day = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"))
means <- predict(object = mod.fit1, newdata = pred.data, type = "link", se.fit = TRUE)
means

##Wald intervals for effects
lower.logmean <- means$fit + qnorm(alpha/2)*means$se.fit
upper.logmean <- means$fit + qnorm(1 - alpha/2)*means$se.fit
lower.logmean
upper.logmean
mean.wald.ci <- data.frame(pred.data, round(cbind(exp(means$fit), exp(lower.logmean), exp(upper.logmean)), digits = 2))
mean.wald.ci
```

#4.5.13(d)
#The model utility test in simple linear regression involves the null hypothesis H0: b1 = 0, according to which there is no useful linear relation between y and the predictor x. In MLR we test the hypothesis H0: b1 = 0, b2 = 0,..., bp = 0, which says that there is no useful linear relationship between y and any of the p predictors. If at least one of these b’s is not 0, the model is deemed useful. 

#4.5.16(a)
```{r}
dt <- read.csv(file  = "dt.csv")
dt <- as.data.frame(dt)
dt
mod.fit2 <- glm(ofp ~ hosp + numchron + gender + school + privins + health_excellent + health_poor, family="poisson", data=dt)
summary(mod.fit2)
```

#4.5.16(b)
```{r}
library(sandwich)
library(lmtest)
coeftest(mod.fit2, vcov = sandwich)
```
#All regressors are still significant but the standard errors seem to be more appropriate. 

#4.5.16(c)
```{r}
library(pscl)
mod.zero <- zeroinfl(ofp ~ ., data = dt, dist = "negbin")
summary(mod.zero)
```
#The excess zeros are generated by a separate process from the count values and that the excess zeros can be modeled independently, which zero-inflated poisson regression is used to model count data that has an excess of zero counts. 

#4.5.18
```{r}
Years <- c(12,12,32,20,20,27,23,19,23,26,21,3,8,35,2,19,8,25,33,35)
Salamanders <- c(3,4,8,6,10,5,4,7,2,8,6,0,2,6,1,5,1,5,4,10)
Salamander <- data.frame(Years,Salamanders)
Salamander

mod.fit5 <- glm(Salamanders ~ Years, family="poisson", data=Salamander)
summary(mod.fit5)


#Likelihood ratio test
anova(mod.fit5)

mean(Salamander$Salamanders)
var(Salamander$Salamanders)

# Frequencies
table(Salamander$Salamanders) #Note that y = 0, 1, ..., 8 all have positive counts
rel.freq <- table(Salamander$Salamanders)/length(Salamander$Salamanders)
rel.freq2 <- c(rel.freq, rep(0, times = 6))

y <- 0:15
prob <- round(dpois(x = y, lambda = mean(Salamander$Salamanders)), 4)

# Observed and Poisson
data.frame(y, prob, rel.freq = rel.freq2)

n <- length(Salamander$Salamanders)
mu.hat <- mean(Salamander$Salamanders)

# Wald
mu.hat + qnorm(p = c(alpha/2, 1 - alpha/2))*sqrt(mu.hat/n)

plot(x = Years, y = Salamanders)
curve(expr = predict(object = mod.fit5, newdata = data.frame(Years = x), type = "response"), col = "blue", add = TRUE)
```
#After possion model specification, we success to reject the null hypothesis that there is no relationship between salamander populations and controlled burn. 


#4.5.25
```{r}
dehart <- read.csv(file  = "DeHartSimplified.csv")
dehart <- as.data.frame(dehart)
dehart

# Reducing data to first Saturday
saturday <- dehart[dehart$dayweek == 6, c(1,4,7,8)] 
head(round(x = saturday, digits = 3))
dim(saturday)
mod.neg <- glm(formula = numall ~ negevent, family = poisson(link = "log"), data = saturday)
summary(mod.neg)

# Plot of dependent variable vs poisson distribution
rel.freq <- table(factor(saturday$numall, levels=0:21))/length(saturday$numall)
y <- 0:21; prob <- round(dpois(y, mean(saturday$numall)), 4)
plot(y-0.1, prob, type="h", ylab="Probability", xlab="# of Drinks",
     main="Observed Data vs Poisson")
axis(side=1, at=seq(0,22,2)); lines(y+0.1, y=rel.freq, type="h", col='red')
legend(10,0.2, c("Poisson", "Observed"), col=c('black','red'),
       lty=c('solid','solid'), bty='n')

# Fit Poisson model
mod.neg2 <- glm(numall ~ nrel + rosn + nrel:rosn, family=poisson(link="log"), data=dehart)
summary(mod.neg2)
# Coefficients
100*(exp(mod.neg2$coefficients)-1)
#Likelihood ratio test
anova(mod.neg2)
```
#After possion model specification, we fail to reject the null hypothesis that there is no relationship between negative romantic events and drinking rate. Similarly, we found no significant indication that negative relationship events would have a stronger impact on individuals with a lower self esteem. 
